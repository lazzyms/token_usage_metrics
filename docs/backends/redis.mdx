---
title: Redis Backend
description: "Fast in-memory token usage tracking with Redis"
icon: "bolt"
---

## Overview

Redis backend provides the fastest write performance with in-memory storage. Ideal for high-throughput applications where low latency is critical.

<CardGroup cols={2}>
  <Card title="Performance" icon="gauge-high">
    ~10,000 writes/sec
  </Card>
  <Card title="Latency" icon="clock">
    &lt;5ms (p99)
  </Card>
</CardGroup>

## Installation

```bash
uv add token-usage-metrics[redis]
```

## Setup

### Start Redis with Docker

```bash
# Basic setup
docker run -d -p 6379:6379 redis:7-alpine

# With persistence
docker run -d -p 6379:6379 \
  -v redis-data:/data \
  redis:7-alpine redis-server --appendonly yes
```

### Production Options

<Tabs>
  <Tab title="AWS ElastiCache">
    ```bash
    redis://your-cluster.cache.amazonaws.com:6379/0
    ```
  </Tab>

{" "}

<Tab title="Redis Cloud">
  ```bash
  redis://default:password@redis-12345.c1.region.cloud.redislabs.com:12345 ```
</Tab>

  <Tab title="Azure Cache">
    ```bash
    rediss://your-cache.redis.cache.windows.net:6380
    ```
  </Tab>
</Tabs>

## Usage Example

### Basic Usage

```python
import asyncio
from token_usage_metrics import TokenUsageClient
from token_usage_metrics.config import Settings

async def main():
  # Initialize with explicit Settings
  settings = Settings(backend="redis", redis_url="redis://localhost:6379/0")
  client = await TokenUsageClient.from_settings(settings)

  # Log token usage
  await client.log(
    project="chatbot",
    request_type="chat",
    input_tokens=100,
    output_tokens=50
  )

  # Query events
  events, _ = await client.query(project="chatbot")
  print(f"Total events: {len(events)}")

  await client.aclose()

asyncio.run(main())
```

### With Metadata

```python
from token_usage_metrics import TokenUsageClient
from token_usage_metrics.config import Settings

async def main():
  settings = Settings(backend="redis", redis_url="redis://localhost:6379/0")
  client = await TokenUsageClient.from_settings(settings)

  # Log with custom metadata
  await client.log(
    "my_app", "chat",
    input_tokens=150,
    output_tokens=75,
    metadata={
      "model": "gpt-4",
      "user_id": "alice",
      "session": "abc123"
    }
  )

  await client.aclose()
```

### Batch Logging

```python
from token_usage_metrics import TokenUsageClient
from token_usage_metrics.config import Settings

async def main():
  settings = Settings(backend="redis", redis_url="redis://localhost:6379/0")
  client = await TokenUsageClient.from_settings(settings)

  # Log multiple events efficiently
  events = [
    {"project": "app1", "type": "chat", "input_tokens": 100, "output_tokens": 50},
    {"project": "app1", "type": "chat", "input_tokens": 120, "output_tokens": 60},
    {"project": "app2", "type": "embedding", "input_tokens": 200, "output_tokens": 0},
  ]

  for event in events:
    await client.log(
      event["project"],
      event["type"],
      input_tokens=event["input_tokens"],
      output_tokens=event["output_tokens"]
    )

  # Force flush to ensure all events are written
  await client.flush()

  await client.aclose()
```

### Query and Aggregate

```python
from datetime import datetime, timedelta, timezone
from token_usage_metrics import TokenUsageClient
from token_usage_metrics.config import Settings

async def main():
  settings = Settings(backend="redis", redis_url="redis://localhost:6379/0")
  client = await TokenUsageClient.from_settings(settings)

  # Query recent events
  events, cursor = await client.query(
    project="chatbot",
    limit=100
  )

  # Get daily aggregates
  daily_stats = await client.aggregate(
    group_by="day",
    time_from=datetime.now(timezone.utc) - timedelta(days=7),
    project="chatbot"
  )

  print("Daily Usage:")
  for bucket in daily_stats:
    print(f"{bucket.start.date()}: {bucket.metrics['total_tokens']} tokens")

  await client.aclose()
```

## Configuration

### Environment Variables

```bash
export TUM_BACKEND=redis
export TUM_REDIS_URL=redis://localhost:6379/0
export TUM_BUFFER_SIZE=1000
export TUM_FLUSH_INTERVAL=1.0
```

### Using Settings Object

```python
from token_usage_metrics import TokenUsageClient
from token_usage_metrics.config import Settings

settings = Settings(
  backend="redis",
  redis_url="redis://localhost:6379/0",

  # Performance tuning
  buffer_size=1000,
  flush_interval=1.0,
  flush_batch_size=200,

  # Connection pooling
  redis_pool_size=10,
)

client = await TokenUsageClient.from_settings(settings)
```

## Data Retention

Redis data persists until explicitly deleted. Implement cleanup for old data:

```python
from datetime import datetime, timedelta, timezone
from token_usage_metrics import DeleteOptions

async def cleanup_old_data(client):
    """Delete data older than 90 days"""
    cutoff = datetime.now(timezone.utc) - timedelta(days=90)

    options = DeleteOptions(
        project_name="my_app",
        time_to=cutoff,
        include_aggregates=True
    )

    result = await client.delete_project(options)
    print(f"Deleted {result.events_deleted} events")
```

## Best Practices

<AccordionGroup>
  <Accordion title="Use Connection Pooling">
    Always configure `redis_pool_size` for production to handle concurrent operations efficiently.
    
    ```python
    settings = Settings(
        backend="redis",
        redis_url="redis://localhost:6379/0",
        redis_pool_size=20
    )
    ```
  </Accordion>

{" "}

<Accordion title="Enable Persistence">
  Use AOF or RDB for data durability: ```bash docker run -d -p 6379:6379 \
  redis:7-alpine redis-server --appendonly yes ```
</Accordion>

  <Accordion title="Monitor Memory Usage">
    Check Redis memory periodically:
    
    ```bash
    redis-cli INFO memory
    ```
  </Accordion>
</AccordionGroup>

<Card title="Next: PostgreSQL" icon="database" href="/backends/postgres">
  Learn about PostgreSQL backend for long-term storage
</Card>
